{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대용량 json 파일 구조 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%cmd\n",
    "head C:/Users/default.DESKTOP-FVT3076/Desktop/data/data.json\n",
    "#안되니까 git bash로 확인\n",
    "#한줄 데이터라 byte 단위로 json 구조 확인\n",
    "head $ head data.json -c 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter notebook --generate-config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#데이터가 너무 커서 블록단위로 메모리에 올리는 ijson 사용\n",
    "import pandas as pd\n",
    "import ijson\n",
    "\n",
    "f = open(f\"./data.json\", encoding=\"UTF-8\")\n",
    "objects = ijson.items(f, 'item')\n",
    "origin = pd.DataFrame(objects)\n",
    "# df = pd.read_csv('./data/성북구_음식점.csv', sep=',', encoding='CP949')\n",
    "\n",
    "# df = df[['업소명', '소재지도로명', '업태명', '주된음식', '행정동명', '소재지전화번호']]\n",
    "# df.columns = ['name', 'address', 'cate1', 'cate2', 'dong', 'phone']\n",
    "# df = df.drop_duplicates(['name'], keep='first')\n",
    "# # 네이버 지도 검색창에 [~동 @@식당]으로 검색해 정확도를 높여야 합니다. 검색어를 미리 설정해줍시다.\n",
    "# df['cate_mix'] = df['cate1'] + df['cate2']\n",
    "print(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,df.shape[0]):\n",
    "    if df.iloc[i]['area'] is None or df.iloc[i,-2] != df.iloc[i,-2]: #nan 찾는 방법\n",
    "        df.iloc[i,-2] = df.iloc[i]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.at[90,'google_keyword'] = df.iloc[90]['name']\n",
    "# df.at[90,'google_keyword']\n",
    "origin.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 셋 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 키워드 삭제\n",
    "origin = origin.drop_duplicates(['google_keyword'], keep='first') # 4371 사라짐\n",
    "print(origin['google_keyword'].value_counts())\n",
    "print(origin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 칼럼\n",
    "origin = origin[['name','area', 'address','latitude','longitude','google_keyword']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#구글맵용 가게정보 data 5등분 하기\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "start = 0\n",
    "for i in range(1, 6):\n",
    "    end = (df1.shape[0]//5) * i\n",
    "    temp = df1[start:end]\n",
    "    file = 'GoogleMap용_가게정보_part_'+str(i) +'.csv'\n",
    "    temp.to_csv(file, encoding='utf-8')\n",
    "    print(file)\n",
    "    start = end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#scweet용 가게정보 data 5등분 하기\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "df = df.drop_duplicates(['name'], keep='first') # 4371 사라짐\n",
    "\n",
    "start = 0\n",
    "for i in range(1, 6):\n",
    "    end = (df.shape[0]//5) * i\n",
    "    df1 = df['name'][start:end]\n",
    "    file = 'scweet용_가게정보_' + str(end) + '행까지.csv'\n",
    "    df1.to_csv(file, encoding='utf-8')\n",
    "    print(file)\n",
    "    start = end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ipynb to py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script naver_search_api.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas list column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../outputs/BERT용 data/BERT_0~20000행_2021-10-03.csv', sep=',', encoding='utf-8')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.loc[~df['google_review_txt'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "df['google_review_txt'] = df['google_review_txt'].apply(literal_eval)\n",
    "df['google_review_txt'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤링 결과 합치기 & 리뷰없는 맛집은 ES에도 넣지 않는다\n",
    "### 두 데이터프레임에서 같은 keyword 찾기\n",
    "* BERT용에서 리뷰없는거 지우고 그 keyword로 origin 데이터에서 contains로 찾아낸다\n",
    "* contains 비교용 리스트 만들때 특수문자 제거해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ijson\n",
    "\n",
    "f = open(f\"./data.json\", encoding=\"UTF-8\")\n",
    "objects = ijson.items(f, 'item')\n",
    "origin = pd.DataFrame(objects)\n",
    "\n",
    "origin[\"name\"] = origin[\"name\"].str.replace('&amp;', '') # str로 Series를 하나하나 파싱해서 지워야함\n",
    "origin['google_keyword'] = origin['area'] + origin['name']\n",
    "origin = origin.loc[~origin['google_keyword'].isnull()]\n",
    "origin = origin.drop_duplicates(['google_keyword'], keep='first') # 4371 사라짐\n",
    "origin = origin[['name','area', 'address','latitude','longitude','google_keyword']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./BERT_30000~40000행_2021-10-06.csv', sep=',', encoding='utf-8')\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "df[\"google_keyword\"] = df[\"google_keyword\"].str.replace('&amp;', '') # str로 Series를 하나하나 파싱해서 지워야함\n",
    "df[\"google_keyword\"] = df[\"google_keyword\"].str.replace(pat=r'[^\\w]', repl=r'', regex=True)\n",
    "df = df.loc[~df['google_review_txt'].isnull()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lis = df['google_keyword'].tolist()\n",
    "listemp = '|'.join(lis)\n",
    "result1 = origin[origin['google_keyword'].str.contains(listemp)]\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'ES_part_17.csv'\n",
    "result1.to_csv(file, encoding='utf-8')\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = df[df['google_keyword'].str.contains(\"?\")].index\n",
    "print(idx)\n",
    "# df = df.drop(idx)\n",
    "# df[df['google_keyword'].str.contains(\"롯데월드몰Wetzel's \")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv 파일 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "input_file = r'C:\\Users\\default.DESKTOP-FVT3076\\Desktop\\저장 데이터\\null처리후' # csv파일들이 있는 디렉토리 위치\n",
    "output_file = r'C:\\Users\\default.DESKTOP-FVT3076\\Desktop\\저장 데이터\\null처리후\\ES_total.csv' # 병합하고 저장하려는 파일명\n",
    "\n",
    "allFile_list = glob.glob(os.path.join(input_file, 'ES_part_*')) # glob함수로 sales_로 시작하는 파일들을 모은다\n",
    "# print(allFile_list)\n",
    "allData = [] # 읽어 들인 csv파일 내용을 저장할 빈 리스트를 하나 만든다\n",
    "for file in allFile_list:\n",
    "    df = pd.read_csv(file) # for구문으로 csv파일들을 읽어 들인다\n",
    "    allData.append(df) # 빈 리스트에 읽어 들인 내용을 추가한다\n",
    "print(allData)\n",
    "dataCombine = pd.concat(allData, axis=0, ignore_index=True) # concat함수를 이용해서 리스트의 내용을 병합\n",
    "# axis=0은 수직으로 병합함. axis=1은 수평. ignore_index=True는 인데스 값이 기존 순서를 무시하고 순서대로 정렬되도록 한다.\n",
    "dataCombine.to_csv(output_file, index=False) # to_csv함수로 저장한다. 인데스를 빼려면 False로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCombine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## send to ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../outputs/ES용 data/ES_0~20000행_2021-10-03.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"name\"] = df[\"name\"].str.replace('&amp;', '') # str로 Series를 하나하나 파싱해서 지워야함\n",
    "df[df['name'].str.contains(\"&amp;\")] # check해서 아무것도 안나오면 정상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def setStore(i, store):\n",
    "    \n",
    "    try:\n",
    "        es = Elasticsearch(\n",
    "            hosts=[{'host':'3.34.51.207', 'port':'9200'}],\n",
    "            http_auth=('elastic','msg!234'))\n",
    "        es.index(index=\"msg\", document=store)\n",
    "    except:\n",
    "        print(store)\n",
    "\n",
    "for d in df.values:\n",
    "#     print(d)\n",
    "    store = {\n",
    "        \"name\" : d[1],\n",
    "        \"area\" : d[2],\n",
    "        \"address\" : d[3],\n",
    "        \"latitude\" : d[4],\n",
    "        \"longitude\" : d[5]\n",
    "    }  \n",
    "    \n",
    "    setStore(d[0],store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
